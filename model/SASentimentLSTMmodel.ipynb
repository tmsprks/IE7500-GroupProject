{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "4428fb1a", "cell_type": "markdown", "source": "# LSTM Sentiment Model with SASentimentModel Interface", "metadata": {}}, {"id": "35b3607c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import pandas as pd\nimport numpy as np\nimport re\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\n", "outputs": []}, {"id": "ed199458", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Load and clean dataset\ndf = pd.read_csv(\"train_60K.csv\")\ndf.columns = [\"sentiment\", \"short_review\", \"full_review\"]\ndf.dropna(subset=[\"full_review\"], inplace=True)\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndf[\"full_review\"] = df[\"full_review\"].astype(str).apply(clean_text)\ndf[\"sentiment\"] = df[\"sentiment\"].replace({1: 0, 2: 1})\n\nX = df[\"full_review\"]\ny = df[\"sentiment\"]\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)", "outputs": []}, {"id": "8aa4422a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from abc import ABC, abstractmethod\n\nclass SASentimentModel(ABC):\n    def __init__(self, vocab_size=10000, embedding_dim=64, sequence_length=200, batch_size=128, epochs=5):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.epochs = epochs\n\n        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size, oov_token=\"<OOV>\")\n        self.model = None\n        self.history = None\n\n    @abstractmethod\n    def register(self, X_train, y_train, X_val, y_val, X_test, y_test):\n        pass\n\n    @abstractmethod\n    def preprocess(self):\n        pass\n\n    @abstractmethod\n    def fit(self):\n        pass\n\n    @abstractmethod\n    def predict(self):\n        pass\n\n    @abstractmethod\n    def evaluate(self):\n        pass\n\n    @abstractmethod\n    def summary(self):\n        pass\n\n    def run(self):\n        self.register(self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test)\n        self.preprocess()\n        self.fit()\n        self.summary()\n        self.predict()\n        self.evaluate()\n", "outputs": []}, {"id": "49f27f89", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "class LSTMSentimentModel(SASentimentModel):\n    def register(self, X_train, y_train, X_val, y_val, X_test, y_test):\n        self.X_train, self.y_train = X_train, y_train\n        self.X_val, self.y_val = X_val, y_val\n        self.X_test, self.y_test = X_test, y_test\n\n    def preprocess(self):\n        self.tokenizer.fit_on_texts(self.X_train)\n        self.X_train_seq = tf.keras.preprocessing.sequence.pad_sequences(\n            self.tokenizer.texts_to_sequences(self.X_train), maxlen=self.sequence_length, padding=\"post\")\n        self.X_val_seq = tf.keras.preprocessing.sequence.pad_sequences(\n            self.tokenizer.texts_to_sequences(self.X_val), maxlen=self.sequence_length, padding=\"post\")\n        self.X_test_seq = tf.keras.preprocessing.sequence.pad_sequences(\n            self.tokenizer.texts_to_sequences(self.X_test), maxlen=self.sequence_length, padding=\"post\")\n\n    def fit(self):\n        self.model = tf.keras.Sequential([\n            tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim, input_length=self.sequence_length),\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n            tf.keras.layers.Dropout(0.5),\n            tf.keras.layers.Dense(64, activation=\"relu\"),\n            tf.keras.layers.Dense(1, activation=\"sigmoid\")\n        ])\n        self.model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n        self.history = self.model.fit(\n            self.X_train_seq, self.y_train,\n            validation_data=(self.X_val_seq, self.y_val),\n            batch_size=self.batch_size,\n            epochs=self.epochs,\n            verbose=1\n        )\n\n    def predict(self):\n        self.y_pred = (self.model.predict(self.X_test_seq) > 0.5).astype(int).flatten()\n        print(classification_report(self.y_test, self.y_pred))\n\n    def evaluate(self):\n        loss, acc = self.model.evaluate(self.X_test_seq, self.y_test)\n        print(f\"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n        f1 = f1_score(self.y_test, self.y_pred)\n        print(f\"F1 Score: {f1:.4f}\")\n        cm = confusion_matrix(self.y_test, self.y_pred)\n        print(\"Confusion Matrix:\")\n        print(cm)\n\n    def summary(self):\n        self.model.summary()\n", "outputs": []}, {"id": "6adf0486", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "model = LSTMSentimentModel()\nmodel.register(X_train, y_train, X_val, y_val, X_test, y_test)\nmodel.run()", "outputs": []}, {"id": "295ab295", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Visualize training history\ndef plot_history(history):\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_history(model.history)", "outputs": []}]}