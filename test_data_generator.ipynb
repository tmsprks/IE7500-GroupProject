{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afec2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\proj\\venv-nlp-project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sa_data_factory import SADataFactory\n",
    "from data_generator import DataGenerator\n",
    "from kaggle_data_loader import KaggleDataLoader\n",
    "from kaggle_dataset import KaggleDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06265db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hOm3b\\.cache\\kagglehub\\datasets\\kritanjalijain\\amazon-reviews\\versions\\2\n",
      "C:\\Users\\hOm3b\\.cache\\kagglehub\\datasets\\kritanjalijain\\amazon-reviews\\versions\\2\n",
      "['amazon_review_polarity_csv.tgz', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kaggle_path = \"kritanjalijain/amazon-reviews\"\n",
    "column_names = [KaggleDataSet.POLARITY_COLUMN_NAME, \n",
    "                KaggleDataSet.TITLE_COLUMN_NAME, \n",
    "                KaggleDataSet.REVIEW_COLUMN_NAME]\n",
    "kaggle_data_loader = KaggleDataLoader(kaggle_path)\n",
    "kaggle_data_loader.load_data(column_names)\n",
    "\n",
    "kaggle_dataset = KaggleDataSet(kaggle_data_loader)\n",
    "train_df = kaggle_dataset.get_train_df()\n",
    "test_df = kaggle_dataset.get_test_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa43543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train_2K.csv from train dataset\n",
      "Label counts:\n",
      " Label\n",
      "1    1440345\n",
      "2    1439655\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'train_2K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    1000\n",
      "2    1000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (2000, 3)\n",
      "\n",
      "Processing train_50K.csv from train dataset\n",
      "Label counts:\n",
      " Label\n",
      "1    1440345\n",
      "2    1439655\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'train_50K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    25000\n",
      "2    25000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (50000, 3)\n",
      "\n",
      "Processing train_80K.csv from train dataset\n",
      "Label counts:\n",
      " Label\n",
      "1    1440345\n",
      "2    1439655\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'train_80K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    40000\n",
      "2    40000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (80000, 3)\n",
      "\n",
      "Processing test_1K.csv from test dataset\n",
      "Label counts:\n",
      " Label\n",
      "2    200000\n",
      "1    200000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'test_1K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    500\n",
      "2    500\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (1000, 3)\n",
      "\n",
      "Processing test_10K.csv from test dataset\n",
      "Label counts:\n",
      " Label\n",
      "2    200000\n",
      "1    200000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'test_10K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    5000\n",
      "2    5000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (10000, 3)\n",
      "\n",
      "Processing test_20K.csv from test dataset\n",
      "Label counts:\n",
      " Label\n",
      "2    200000\n",
      "1    200000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'test_20K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    10000\n",
      "2    10000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (20000, 3)\n",
      "\n",
      "Processing validate_1K.csv from validation dataset\n",
      "Label counts:\n",
      " Label\n",
      "2    360345\n",
      "1    359655\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'validate_1K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    500\n",
      "2    500\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (1000, 3)\n",
      "\n",
      "Processing validate_10K.csv from validation dataset\n",
      "Label counts:\n",
      " Label\n",
      "2    360345\n",
      "1    359655\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'validate_10K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    5000\n",
      "2    5000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (10000, 3)\n",
      "\n",
      "Processing validate_20K.csv from validation dataset\n",
      "Label counts:\n",
      " Label\n",
      "2    360345\n",
      "1    359655\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset saved to 'validate_20K.csv', ensure uniqueness across dataset type: True, randomize samples: False\n",
      "Downsampled dataset label counts:\n",
      " Label\n",
      "1    10000\n",
      "2    10000\n",
      "Name: count, dtype: int64\n",
      "Downsampled dataset size: (20000, 3)\n",
      "\n",
      "\n",
      "File generations completed\n"
     ]
    }
   ],
   "source": [
    "sa_data_factory = SADataFactory(kaggle_dataset)   \n",
    "generator = DataGenerator(sa_data_factory)\n",
    "results = generator.generate_datasets(ensure_uniqueness_across_same_dataset_type=True, randomize_samples=False)\n",
    "print(\"\\n\\nFile generations completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b826d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data files generated: 9\n",
      "\n",
      "\n",
      "Generated train_2K.csv, ensure uniqueness True, randomize samples False with 2000 rows\n",
      "   Label                           Title  \\\n",
      "0      1  Universal cry (Nasio Fontaine)   \n",
      "1      1                          rehash   \n",
      "\n",
      "                                              Review  \n",
      "0  I have not recieve this item purchased on 11/2...  \n",
      "1  total rehash of other ideas from thinkers some...  \n",
      "\n",
      "Generated train_50K.csv, ensure uniqueness True, randomize samples False with 50000 rows\n",
      "   Label                           Title  \\\n",
      "0      1  Give Credit Where Credit's Due   \n",
      "1      1                Great frying pan   \n",
      "\n",
      "                                              Review  \n",
      "0  I'm here to tell everyone the truth about this...  \n",
      "1  I am very happy with this purchase. It is the ...  \n",
      "\n",
      "Generated train_80K.csv, ensure uniqueness True, randomize samples False with 80000 rows\n",
      "   Label         Title                                             Review\n",
      "0      1    not scarce  \"premium price due to scarcity\" ?????This colo...\n",
      "1      1  Please avoid  This was one of the movie streamed online and ...\n",
      "\n",
      "Generated test_1K.csv, ensure uniqueness True, randomize samples False with 1000 rows\n",
      "   Label                  Title  \\\n",
      "0      1  So where's the \"fun\"?   \n",
      "1      1        product quality   \n",
      "\n",
      "                                              Review  \n",
      "0  What kind of a cheap @$$ game is this?! It's n...  \n",
      "1  it is impossible to cram it down any drain, I ...  \n",
      "\n",
      "Generated test_10K.csv, ensure uniqueness True, randomize samples False with 10000 rows\n",
      "   Label                     Title  \\\n",
      "0      1            Colored People   \n",
      "1      1  Don't waste your time!!!   \n",
      "\n",
      "                                              Review  \n",
      "0  The book \"Colored People\" is a good book for a...  \n",
      "1  This is an example of one bad apple. A $5.00 b...  \n",
      "\n",
      "Generated test_20K.csv, ensure uniqueness True, randomize samples False with 20000 rows\n",
      "   Label                                              Title  \\\n",
      "0      1                      I haven't got my magazine yet   \n",
      "1      1  Bruce, you couldnt have picked a worse scrpit ...   \n",
      "\n",
      "                                              Review  \n",
      "0  I never received a copy of this magazine i am ...  \n",
      "1  This was honestly one of the worst movies i ha...  \n",
      "\n",
      "Generated validate_1K.csv, ensure uniqueness True, randomize samples False with 1000 rows\n",
      "   Label                                       Title  \\\n",
      "0      1                                    Bad Wire   \n",
      "1      1  Palm has already ripped on the right hand.   \n",
      "\n",
      "                                              Review  \n",
      "0  For 6 hours I thought it was the way I was hoo...  \n",
      "1  I used these for work and about after a month,...  \n",
      "\n",
      "Generated validate_10K.csv, ensure uniqueness True, randomize samples False with 10000 rows\n",
      "   Label                  Title  \\\n",
      "0      1  Stay away from Amazon   \n",
      "1      1      Armageddon - bad!   \n",
      "\n",
      "                                              Review  \n",
      "0  Amazon still doesn't know how to ship spindles...  \n",
      "1  Truly one of the worst films ever made! Michae...  \n",
      "\n",
      "Generated validate_20K.csv, ensure uniqueness True, randomize samples False with 20000 rows\n",
      "   Label                       Title  \\\n",
      "0      1                not reliable   \n",
      "1      1  No suspension of disbelief   \n",
      "\n",
      "                                              Review  \n",
      "0  Lets face it, we buy a watch to tell the time!...  \n",
      "1  This is one of the worst movies I've ever seen...  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of data files generated: {len(results)}\\n\")\n",
    "for output_file, ensure_uniqueness, randomize_samples, df in results:\n",
    "    print(f\"\\nGenerated {output_file}, ensure uniqueness {ensure_uniqueness}, randomize samples {randomize_samples} with {len(df)} rows\")\n",
    "    print(df.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
