{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987907a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\Proj\\venv-nlp-proj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from SASentimentModel import SASentimentModel\n",
    "from kaggle_dataset import KaggleDataSet\n",
    "\n",
    "from sa_model_pipeline import SAModelPipeline\n",
    "from sa_model_config_loader import SAModelConfigLoader\n",
    "from sa_data_loader import SADataLoader\n",
    "from sa_model_params import SAModelParams\n",
    "from SASelfAttentionModel import SASelfAttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5863573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698a3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    ### \n",
    "    ### To run your model, \n",
    "    ###\n",
    "    ### 1) Change model_module_name to match your model's module name, i.e., the file name\n",
    "    ### 2) Change model_class_name to match  your model's class name like SASelfAttentionModel\n",
    "    ### 3) Look for this line at the bottom of the method,\n",
    "    ###    sa_sentiment_model = SASelfAttentionModel(sa_model_params)\n",
    "    ###    Change SASelfAttentionModel to match your model's class.\n",
    "    ###    That's it.\n",
    "    ### 4) Then run this notebook from start to finish.\n",
    "    ###\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### Modify model_module_name and model_class_name to match your model's \n",
    "    ### module and class name and then change the model instance to match your model class below\n",
    "    ###\n",
    "    model_module_name = \"SASelfAttentionModel\"\n",
    "    model_class_name = \"SASelfAttentionModel\"\n",
    "\n",
    "    ### Load the model configuration and then find the model specified above\n",
    "    sa_model_config_loader = SAModelConfigLoader()\n",
    "    list_of_model_config = sa_model_config_loader.load_model_config()\n",
    "    model_config = sa_model_config_loader.find_model_config(model_module_name, model_class_name)\n",
    "    \n",
    "    ###\n",
    "    ### Construct the path to the train, test and validation CSV file specified in the model config file\n",
    "    ### for the given model\n",
    "    ###\n",
    "    cwd = os.getcwd()        \n",
    "    train_file = model_config.get_model_train_csv_file_name()\n",
    "    test_file = model_config.get_model_test_csv_file_name()\n",
    "    validation_file = model_config.get_model_validation_csv_file_name()\n",
    "\n",
    "    path_to_train_csv_file = os.path.join(cwd, train_file)\n",
    "    path_to_test_csv_file = os.path.join(cwd, test_file)\n",
    "    path_to_validation_csv_file = os.path.join(cwd, validation_file)\n",
    "\n",
    "    ### Load the train, test and validation data file specific to the model\n",
    "    sa_data_loader = SADataLoader(path_to_train_csv_file, path_to_test_csv_file, path_to_validation_csv_file)\n",
    "    sa_data_loader.load_data(KaggleDataSet.get_kaggle_column_names())\n",
    "\n",
    "    ### Construct the model parameter object\n",
    "    sa_model_params = SAModelParams(sa_data_loader, model_config)\n",
    "\n",
    "    logger.info(f\"Start running model: {model_module_name}:{model_class_name}\") \n",
    "\n",
    "    ###\n",
    "    ### CHANGE THE MODEL TO YOUR MODEL CLASS!!\n",
    "    ### \n",
    "\n",
    "    sa_sentiment_model = SASelfAttentionModel(sa_model_params)\n",
    "    \n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "\n",
    "    ###\n",
    "    ### Call the SASentimentModel's run() which will run the model pipeline\n",
    "    ###\n",
    "    sa_sentiment_model.run(sa_model_params)\n",
    "\n",
    "    logger.info(f\"Finished running model: {sa_sentiment_model.__class__.__name__}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72349c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform set to: <bound method BinaryLabelTransformer.transform of <binary_label_transformer.BinaryLabelTransformer object at 0x000001341E5AD1C0>>, Type: <class 'method'>\n",
      "Path to train csv file: z:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\Proj\\src\\SA_NLP\\train_60K.csv\n",
      "Path to test csv file: z:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\Proj\\src\\SA_NLP\\test_20K.csv\n",
      "Path to validation csv file: z:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\Proj\\src\\SA_NLP\\validate_20K.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:21:26,407 - INFO - Start running model: SASelfAttentionModel:SASelfAttentionModel\n",
      "2025-06-24 15:21:26,408 - INFO - Calling SASelfAttentionModel.register(): Self Attention Model/SASelfAttentionModel/SASelfAttentionModel/train_60K.csv/test_20K.csv/validate_20K.csv/{'vocab_size': 10000, 'max_features': 20000, 'sequence_length': 500, 'embedding_dim': 20000, 'embed_output': 128, 'epoch': 5, 'batch_size': 32}\n",
      "2025-06-24 15:21:26,408 - INFO - SASelfAttentionModel.register(): Completed\n",
      "2025-06-24 15:21:26,410 - INFO - Calling SASelfAttentionModel.preprocess(): Self Attention Model/SASelfAttentionModel/SASelfAttentionModel/train_60K.csv/test_20K.csv/validate_20K.csv/{'vocab_size': 10000, 'max_features': 20000, 'sequence_length': 500, 'embedding_dim': 20000, 'embed_output': 128, 'epoch': 5, 'batch_size': 32}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60000\n",
      "Test size: 20000\n",
      "Validation size: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:21:27,624 - INFO - SASelfAttentionModel.preprocess(): Completed\n",
      "2025-06-24 15:21:27,624 - INFO - Calling SASelfAttentionModel.fit(): Self Attention Model/SASelfAttentionModel/SASelfAttentionModel/train_60K.csv/test_20K.csv/validate_20K.csv/{'vocab_size': 10000, 'max_features': 20000, 'sequence_length': 500, 'embedding_dim': 20000, 'embed_output': 128, 'epoch': 5, 'batch_size': 32}\n",
      "e:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\Proj\\venv-nlp-proj\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-06-24 15:21:27,664 - INFO - Calling SASelfAttentionModel.fit(): Model compiled\n",
      "2025-06-24 15:21:27,669 - INFO - Calling SASelfAttentionModel.fit(): Fitting model: X_train: 60000, y_train: 60000, X_val: 20000, y_val: 20000,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From e:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\Proj\\venv-nlp-proj\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:21:28,525 - WARNING - From e:\\life\\edu\\NU-DAE\\IE 7500 - NLP\\Proj\\venv-nlp-proj\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 164ms/step - accuracy: 0.7669 - loss: 0.4570 - val_accuracy: 0.8784 - val_loss: 0.2838\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 163ms/step - accuracy: 0.9183 - loss: 0.2129 - val_accuracy: 0.8892 - val_loss: 0.2676\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 168ms/step - accuracy: 0.9522 - loss: 0.1363 - val_accuracy: 0.8829 - val_loss: 0.2946\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 161ms/step - accuracy: 0.9742 - loss: 0.0776 - val_accuracy: 0.8778 - val_loss: 0.3760\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 161ms/step - accuracy: 0.9855 - loss: 0.0456 - val_accuracy: 0.8729 - val_loss: 0.4551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:47:03,963 - INFO - Calling SASelfAttentionModel.fit(): Model fitted\n",
      "2025-06-24 15:47:03,963 - INFO - SASelfAttentionModel.fit(): Completed\n",
      "2025-06-24 15:47:03,963 - INFO - Calling SASelfAttentionModel.summary(): Self Attention Model/SASelfAttentionModel/SASelfAttentionModel/train_60K.csv/test_20K.csv/validate_20K.csv/{'vocab_size': 10000, 'max_features': 20000, 'sequence_length': 500, 'embedding_dim': 20000, 'embed_output': 128, 'epoch': 5, 'batch_size': 32}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sa_self_attention_layer         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,321</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SASelfAttentionLayer</span>)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sa_self_attention_layer         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │         \u001b[38;5;34m8,321\u001b[0m │\n",
       "│ (\u001b[38;5;33mSASelfAttentionLayer\u001b[0m)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,026,376</span> (30.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,026,376\u001b[0m (30.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,675,458</span> (10.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,675,458\u001b[0m (10.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,350,918</span> (20.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m5,350,918\u001b[0m (20.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:47:03,995 - INFO - SASelfAttentionModel.summary(): Completed\n",
      "2025-06-24 15:47:03,995 - INFO - Calling SASelfAttentionModel.predict(): Self Attention Model/SASelfAttentionModel/SASelfAttentionModel/train_60K.csv/test_20K.csv/validate_20K.csv/{'vocab_size': 10000, 'max_features': 20000, 'sequence_length': 500, 'embedding_dim': 20000, 'embed_output': 128, 'epoch': 5, 'batch_size': 32}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:47:45,127 - INFO - SASelfAttentionModel.predict(): Completed\n",
      "2025-06-24 15:47:45,127 - INFO - Calling SASelfAttentionModel.evaluate(): Self Attention Model/SASelfAttentionModel/SASelfAttentionModel/train_60K.csv/test_20K.csv/validate_20K.csv/{'vocab_size': 10000, 'max_features': 20000, 'sequence_length': 500, 'embedding_dim': 20000, 'embed_output': 128, 'epoch': 5, 'batch_size': 32}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88     10000\n",
      "           1       0.89      0.85      0.87     10000\n",
      "\n",
      "    accuracy                           0.87     20000\n",
      "   macro avg       0.87      0.87      0.87     20000\n",
      "weighted avg       0.87      0.87      0.87     20000\n",
      "\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 43ms/step - accuracy: 0.8838 - loss: 0.3982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:48:11,922 - INFO - SASelfAttentionModel.evaluate(): Completed\n",
      "2025-06-24 15:48:11,922 - INFO - Finished running model: SASelfAttentionModel\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
